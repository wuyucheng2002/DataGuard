{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "def get_data(filepath, isTest=False):\n",
    "    line_list = []\n",
    "    with open(filepath) as lines:\n",
    "        if isTest:\n",
    "            next(lines)\n",
    "        for line in lines:\n",
    "            l = line.split(',')\n",
    "            if '?' in l:\n",
    "                print('yes')\n",
    "            line_list.append(l)\n",
    "\n",
    "    data = pd.DataFrame(line_list,\n",
    "                    columns=['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "                             'marital-status', 'occupation', 'relationship',\n",
    "                             'race', 'sex', 'capital-gain', 'capital-loss','hours-per-week',\n",
    "                             'native-country', 'label'])\n",
    "\n",
    "    # 丢弃最后一行空行\n",
    "    data = data.iloc[:-1, :]\n",
    "\n",
    "    for col in ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']:\n",
    "        data[col] = data[col].astype(int)\n",
    "\n",
    "    for col in ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'native-country', 'sex', 'label']:\n",
    "        data[col] = data[col].astype(str)\n",
    "        data[col] = data[col].apply(lambda x: x.strip(' '))\n",
    "        data[col] = data[col].apply(lambda x : None if x == '?' else x)\n",
    "\n",
    "    data['label'] = data['label'].apply(lambda l : l.strip())\n",
    "\n",
    "    data.dropna(axis='index', how='any', inplace=True)\n",
    "\n",
    "    data['sex'] = data['sex'].apply(lambda x : 0 if x == 'Male' else 1)\n",
    "\n",
    "    data['label'] = data['label'].apply(lambda x : 0 if x.strip('.') =='<=50K' else 1)\n",
    "\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "data = get_data('./Adult/adult.data', False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "   age         workclass  fnlwgt  education  education-num  \\\n0   39         State-gov   77516  Bachelors             13   \n1   50  Self-emp-not-inc   83311  Bachelors             13   \n2   38           Private  215646    HS-grad              9   \n3   53           Private  234721       11th              7   \n4   28           Private  338409  Bachelors             13   \n\n       marital-status         occupation   relationship   race  sex  \\\n0       Never-married       Adm-clerical  Not-in-family  White    0   \n1  Married-civ-spouse    Exec-managerial        Husband  White    0   \n2            Divorced  Handlers-cleaners  Not-in-family  White    0   \n3  Married-civ-spouse  Handlers-cleaners        Husband  Black    0   \n4  Married-civ-spouse     Prof-specialty           Wife  Black    1   \n\n   capital-gain  capital-loss  hours-per-week native-country  label  \n0          2174             0              40  United-States      0  \n1             0             0              13  United-States      0  \n2             0             0              40  United-States      0  \n3             0             0              40  United-States      0  \n4             0             0              40           Cuba      0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>workclass</th>\n      <th>fnlwgt</th>\n      <th>education</th>\n      <th>education-num</th>\n      <th>marital-status</th>\n      <th>occupation</th>\n      <th>relationship</th>\n      <th>race</th>\n      <th>sex</th>\n      <th>capital-gain</th>\n      <th>capital-loss</th>\n      <th>hours-per-week</th>\n      <th>native-country</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>39</td>\n      <td>State-gov</td>\n      <td>77516</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Never-married</td>\n      <td>Adm-clerical</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>0</td>\n      <td>2174</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>50</td>\n      <td>Self-emp-not-inc</td>\n      <td>83311</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Married-civ-spouse</td>\n      <td>Exec-managerial</td>\n      <td>Husband</td>\n      <td>White</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>13</td>\n      <td>United-States</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>38</td>\n      <td>Private</td>\n      <td>215646</td>\n      <td>HS-grad</td>\n      <td>9</td>\n      <td>Divorced</td>\n      <td>Handlers-cleaners</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>53</td>\n      <td>Private</td>\n      <td>234721</td>\n      <td>11th</td>\n      <td>7</td>\n      <td>Married-civ-spouse</td>\n      <td>Handlers-cleaners</td>\n      <td>Husband</td>\n      <td>Black</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>28</td>\n      <td>Private</td>\n      <td>338409</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Married-civ-spouse</td>\n      <td>Prof-specialty</td>\n      <td>Wife</td>\n      <td>Black</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>Cuba</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "train = data.copy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "X_train_raw = train.drop(columns=['label'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "       age         workclass  fnlwgt   education  education-num  \\\n0       39         State-gov   77516   Bachelors             13   \n1       50  Self-emp-not-inc   83311   Bachelors             13   \n2       38           Private  215646     HS-grad              9   \n3       53           Private  234721        11th              7   \n4       28           Private  338409   Bachelors             13   \n...    ...               ...     ...         ...            ...   \n32556   27           Private  257302  Assoc-acdm             12   \n32557   40           Private  154374     HS-grad              9   \n32558   58           Private  151910     HS-grad              9   \n32559   22           Private  201490     HS-grad              9   \n32560   52      Self-emp-inc  287927     HS-grad              9   \n\n           marital-status         occupation   relationship   race  sex  \\\n0           Never-married       Adm-clerical  Not-in-family  White    0   \n1      Married-civ-spouse    Exec-managerial        Husband  White    0   \n2                Divorced  Handlers-cleaners  Not-in-family  White    0   \n3      Married-civ-spouse  Handlers-cleaners        Husband  Black    0   \n4      Married-civ-spouse     Prof-specialty           Wife  Black    1   \n...                   ...                ...            ...    ...  ...   \n32556  Married-civ-spouse       Tech-support           Wife  White    1   \n32557  Married-civ-spouse  Machine-op-inspct        Husband  White    0   \n32558             Widowed       Adm-clerical      Unmarried  White    1   \n32559       Never-married       Adm-clerical      Own-child  White    0   \n32560  Married-civ-spouse    Exec-managerial           Wife  White    1   \n\n       capital-gain  capital-loss  hours-per-week native-country  \n0              2174             0              40  United-States  \n1                 0             0              13  United-States  \n2                 0             0              40  United-States  \n3                 0             0              40  United-States  \n4                 0             0              40           Cuba  \n...             ...           ...             ...            ...  \n32556             0             0              38  United-States  \n32557             0             0              40  United-States  \n32558             0             0              40  United-States  \n32559             0             0              20  United-States  \n32560         15024             0              40  United-States  \n\n[30162 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>workclass</th>\n      <th>fnlwgt</th>\n      <th>education</th>\n      <th>education-num</th>\n      <th>marital-status</th>\n      <th>occupation</th>\n      <th>relationship</th>\n      <th>race</th>\n      <th>sex</th>\n      <th>capital-gain</th>\n      <th>capital-loss</th>\n      <th>hours-per-week</th>\n      <th>native-country</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>39</td>\n      <td>State-gov</td>\n      <td>77516</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Never-married</td>\n      <td>Adm-clerical</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>0</td>\n      <td>2174</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>50</td>\n      <td>Self-emp-not-inc</td>\n      <td>83311</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Married-civ-spouse</td>\n      <td>Exec-managerial</td>\n      <td>Husband</td>\n      <td>White</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>13</td>\n      <td>United-States</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>38</td>\n      <td>Private</td>\n      <td>215646</td>\n      <td>HS-grad</td>\n      <td>9</td>\n      <td>Divorced</td>\n      <td>Handlers-cleaners</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>53</td>\n      <td>Private</td>\n      <td>234721</td>\n      <td>11th</td>\n      <td>7</td>\n      <td>Married-civ-spouse</td>\n      <td>Handlers-cleaners</td>\n      <td>Husband</td>\n      <td>Black</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>28</td>\n      <td>Private</td>\n      <td>338409</td>\n      <td>Bachelors</td>\n      <td>13</td>\n      <td>Married-civ-spouse</td>\n      <td>Prof-specialty</td>\n      <td>Wife</td>\n      <td>Black</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>Cuba</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>32556</th>\n      <td>27</td>\n      <td>Private</td>\n      <td>257302</td>\n      <td>Assoc-acdm</td>\n      <td>12</td>\n      <td>Married-civ-spouse</td>\n      <td>Tech-support</td>\n      <td>Wife</td>\n      <td>White</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>38</td>\n      <td>United-States</td>\n    </tr>\n    <tr>\n      <th>32557</th>\n      <td>40</td>\n      <td>Private</td>\n      <td>154374</td>\n      <td>HS-grad</td>\n      <td>9</td>\n      <td>Married-civ-spouse</td>\n      <td>Machine-op-inspct</td>\n      <td>Husband</td>\n      <td>White</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n    </tr>\n    <tr>\n      <th>32558</th>\n      <td>58</td>\n      <td>Private</td>\n      <td>151910</td>\n      <td>HS-grad</td>\n      <td>9</td>\n      <td>Widowed</td>\n      <td>Adm-clerical</td>\n      <td>Unmarried</td>\n      <td>White</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n    </tr>\n    <tr>\n      <th>32559</th>\n      <td>22</td>\n      <td>Private</td>\n      <td>201490</td>\n      <td>HS-grad</td>\n      <td>9</td>\n      <td>Never-married</td>\n      <td>Adm-clerical</td>\n      <td>Own-child</td>\n      <td>White</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>20</td>\n      <td>United-States</td>\n    </tr>\n    <tr>\n      <th>32560</th>\n      <td>52</td>\n      <td>Self-emp-inc</td>\n      <td>287927</td>\n      <td>HS-grad</td>\n      <td>9</td>\n      <td>Married-civ-spouse</td>\n      <td>Exec-managerial</td>\n      <td>Wife</td>\n      <td>White</td>\n      <td>1</td>\n      <td>15024</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n    </tr>\n  </tbody>\n</table>\n<p>30162 rows × 14 columns</p>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_raw"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "y_train = train['label']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "0        0\n1        0\n2        0\n3        0\n4        0\n        ..\n32556    0\n32557    1\n32558    0\n32559    0\n32560    1\nName: label, Length: 30162, dtype: int64"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "num_col = ['age', 'fnlwgt', 'education-num', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "cat_col = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'native-country']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", MinMaxScaler(), num_col),\n",
    "    (\"cat\", OneHotEncoder(), cat_col)])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "X_train = full_pipeline.fit_transform(X_train_raw)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "<30162x103 sparse matrix of type '<class 'numpy.float64'>'\n\twith 345148 stored elements in Compressed Sparse Row format>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, roc_curve\n",
    "from sklearn.base import clone"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# 手动交叉验证\n",
    "def eval_model(model_dic, X_train, y_train, isStratified=True):\n",
    "    if isStratified:\n",
    "        kfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=3407)\n",
    "    else:\n",
    "        kfolds = KFold(n_splits=5, shuffle=True, random_state=3407)\n",
    "\n",
    "    for model_name, model in model_dic.items():\n",
    "        scores = {'accuracy':[],\n",
    "                  'auc':None}\n",
    "        y_pred_prb = np.zeros(shape=y_train.shape)\n",
    "\n",
    "        for train_indices, valid_indices in kfolds.split(X_train, y_train):\n",
    "            clone_clf = clone(model)\n",
    "\n",
    "            X_train_fold = X_train[train_indices, :]\n",
    "            y_train_fold = y_train.iloc[train_indices]\n",
    "            X_valid_fold = X_train[valid_indices, :]\n",
    "            y_valid_fold = y_train.iloc[valid_indices]\n",
    "\n",
    "            clone_clf.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "            y_pred = clone_clf.predict(X_valid_fold)\n",
    "            y_pred_prb[valid_indices] = clone_clf.predict_proba(X_valid_fold)[:, 1]\n",
    "\n",
    "            scores['accuracy'].append(accuracy_score(y_valid_fold, y_pred))\n",
    "\n",
    "        scores['auc'] = roc_auc_score(y_train, y_pred_prb)\n",
    "\n",
    "        print(\"{}分类器的5折交叉验证结果为: \\n accuracy为: {}({})\\n auc为: {}\".format(model_name,\n",
    "                                                                                   np.mean(scores['accuracy']),\n",
    "                                                                                   np.std(scores['accuracy']),\n",
    "                                                                                   scores['auc']))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression(max_iter=1000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "model_dic = {\"log\":log_clf}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log分类器的5折交叉验证结果为: \n",
      " accuracy为: 0.8459318795255626(0.006034754588434064)\n",
      " auc为: 0.9022206688663666\n"
     ]
    }
   ],
   "source": [
    "eval_model(model_dic, X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 测试"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "test_data = get_data('./Adult/adult.test', isTest=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "test = test_data.copy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "X_test_raw = test.drop(columns=['label'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "y_test = test['label']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "X_test = full_pipeline.transform(X_test_raw)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def test_eval(model_fitted_dic, X_test, y_test):\n",
    "    model_scores = {}\n",
    "    for model_name, model in model_fitted_dic.items():\n",
    "        scores = {'accuracy': None,\n",
    "                  'auc': None,\n",
    "                  'roc': None}\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_prb = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        scores['accuracy'] = accuracy_score(y_test, y_pred)\n",
    "        scores['auc'] = roc_auc_score(y_test, y_pred_prb)\n",
    "        scores['roc'] = roc_curve(y_test, y_pred_prb)\n",
    "\n",
    "        print(\"{}分类器在测试集上的结果为: \\n accuracy为: {}\\n auc为: {}\".format(model_name,\n",
    "                                                                                  scores['accuracy'],\n",
    "                                                                                  scores['auc']))\n",
    "        model_scores[model_name] = scores\n",
    "    return model_scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "log_clf_trained = LogisticRegression(max_iter=1000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression(max_iter=1000)",
      "text/html": "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_clf_trained.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "model_fitted_dic = {\"log\": log_clf_trained}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log分类器在测试集上的结果为: \n",
      " accuracy为: 0.8460159362549801\n",
      " auc为: 0.9015326774838219\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'log': {'accuracy': 0.8460159362549801,\n  'auc': 0.9015326774838219,\n  'roc': (array([0.        , 0.        , 0.        , ..., 0.97077465, 0.9709507 ,\n          1.        ]),\n   array([0.00000000e+00, 2.70270270e-04, 2.18918919e-02, ...,\n          1.00000000e+00, 1.00000000e+00, 1.00000000e+00]),\n   array([2.00000000e+00, 9.99999999e-01, 9.99990337e-01, ...,\n          1.28166381e-03, 1.27875945e-03, 6.90591988e-05]))}}"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_eval(model_fitted_dic, X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "<30162x103 sparse matrix of type '<class 'numpy.float64'>'\n\twith 345148 stored elements in Compressed Sparse Row format>"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "<15060x103 sparse matrix of type '<class 'numpy.float64'>'\n\twith 172339 stored elements in Compressed Sparse Row format>"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 使用tensorflow构建的逻辑回归"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "outputs": [],
   "source": [
    "def evaluate(y_pred, y):\n",
    "    ones = tf.ones_like(y_pred)\n",
    "    zeros = tf.zeros_like(y_pred)\n",
    "    output = tf.where(y_pred > 0.5 , ones, zeros)\n",
    "    y = tf.reshape(y, output.shape)\n",
    "    res = tf.reduce_sum(tf.where(y == tf.cast(output, dtype=y.dtype), ones, zeros))/y.shape[0]\n",
    "    return res.numpy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self) -> None:\n",
    "        self.batch_size = 256\n",
    "        self.learn_rate = 2\n",
    "        self.epochs = 200\n",
    "config = Config()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "outputs": [],
   "source": [
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    \"\"\"\n",
    "    以batch size加载X和y\n",
    "    :param data_arrays: (X, y),其中X和y均为array\n",
    "    :param batch_size:\n",
    "    :param is_train: 如果True则会shuffle\n",
    "    :return: dataloader\n",
    "    \"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data_arrays)\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(buffer_size=len(data_arrays), seed=42)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "outputs": [],
   "source": [
    "data_loader = load_array((X_train.toarray(), y_train), config.batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "outputs": [],
   "source": [
    "# model = tf.keras.Sequential()\n",
    "# model.add(tf.keras.layers.Dense(units=2, kernel_initializer=tf.keras.initializers.Zeros(), activation='sigmoid'))\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(units=200, kernel_initializer=tf.keras.initializers.Zeros()))\n",
    "model.add(tf.keras.layers.Dense(units=50, kernel_initializer=tf.keras.initializers.RandomNormal(seed=42),\n",
    "                                activation='softmax'))\n",
    "model.add(tf.keras.layers.Dense(2))\n",
    "model.add(tf.keras.layers.Activation('sigmoid'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=config.learn_rate)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "outputs": [],
   "source": [
    "# def evaluate(y_true, y_pred):\n",
    "#     ones = tf.ones_like(y_pred)\n",
    "#     zeros = tf.zeros_like(y_pred)\n",
    "#     output = tf.where(y_pred > 0.5 , ones, zeros)\n",
    "#     y = tf.reshape(y_true, output.shape)\n",
    "#     res = tf.reduce_sum(tf.where(y == tf.cast(output, dtype=y.dtype), ones, zeros))/y.shape[0]\n",
    "#     return res.numpy()\n",
    "def evaluate(y_true, y_pred):\n",
    "    y_pred = tf.argmax(y_pred, axis=1)\n",
    "    res = tf.equal(tf.cast(y_true, y_pred.dtype), y_pred)\n",
    "    res = np.mean(res)\n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense_46 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "epoch:1, trian_loss:0.56332, train_acc:0.751, test_loss:0.55517, test_acc:0.754\n",
      "epoch:2, trian_loss:0.52205, train_acc:0.757, test_loss:0.42414, test_acc:0.808\n",
      "epoch:3, trian_loss:0.40119, train_acc:0.824, test_loss:0.38736, test_acc:0.829\n",
      "epoch:4, trian_loss:0.37716, train_acc:0.830, test_loss:0.37961, test_acc:0.827\n",
      "epoch:5, trian_loss:0.36921, train_acc:0.834, test_loss:0.37525, test_acc:0.825\n",
      "epoch:6, trian_loss:0.36398, train_acc:0.836, test_loss:0.37359, test_acc:0.822\n",
      "epoch:7, trian_loss:0.36044, train_acc:0.838, test_loss:0.37120, test_acc:0.823\n",
      "epoch:8, trian_loss:0.35722, train_acc:0.839, test_loss:0.36823, test_acc:0.823\n",
      "epoch:9, trian_loss:0.35413, train_acc:0.840, test_loss:0.36745, test_acc:0.822\n",
      "epoch:10, trian_loss:0.35217, train_acc:0.840, test_loss:0.36695, test_acc:0.822\n",
      "epoch:11, trian_loss:0.35072, train_acc:0.840, test_loss:0.36684, test_acc:0.821\n",
      "epoch:12, trian_loss:0.34951, train_acc:0.841, test_loss:0.36589, test_acc:0.821\n",
      "epoch:13, trian_loss:0.34795, train_acc:0.841, test_loss:0.36746, test_acc:0.821\n",
      "epoch:14, trian_loss:0.34723, train_acc:0.842, test_loss:0.36207, test_acc:0.824\n",
      "epoch:15, trian_loss:0.34603, train_acc:0.842, test_loss:0.36449, test_acc:0.822\n",
      "epoch:16, trian_loss:0.34528, train_acc:0.843, test_loss:0.36360, test_acc:0.822\n",
      "epoch:17, trian_loss:0.34474, train_acc:0.843, test_loss:0.36273, test_acc:0.823\n",
      "epoch:18, trian_loss:0.34410, train_acc:0.843, test_loss:0.36452, test_acc:0.821\n",
      "epoch:19, trian_loss:0.34326, train_acc:0.843, test_loss:0.36187, test_acc:0.823\n",
      "epoch:20, trian_loss:0.34282, train_acc:0.843, test_loss:0.35843, test_acc:0.825\n",
      "epoch:21, trian_loss:0.34216, train_acc:0.844, test_loss:0.35974, test_acc:0.824\n",
      "epoch:22, trian_loss:0.34162, train_acc:0.844, test_loss:0.36066, test_acc:0.824\n",
      "epoch:23, trian_loss:0.34115, train_acc:0.844, test_loss:0.35837, test_acc:0.825\n",
      "epoch:24, trian_loss:0.34121, train_acc:0.844, test_loss:0.35544, test_acc:0.826\n",
      "epoch:25, trian_loss:0.34033, train_acc:0.844, test_loss:0.35678, test_acc:0.825\n",
      "epoch:26, trian_loss:0.34009, train_acc:0.845, test_loss:0.35576, test_acc:0.826\n",
      "epoch:27, trian_loss:0.33949, train_acc:0.845, test_loss:0.35725, test_acc:0.825\n",
      "epoch:28, trian_loss:0.33914, train_acc:0.845, test_loss:0.35481, test_acc:0.827\n",
      "epoch:29, trian_loss:0.33856, train_acc:0.845, test_loss:0.35271, test_acc:0.827\n",
      "epoch:30, trian_loss:0.33831, train_acc:0.846, test_loss:0.35431, test_acc:0.827\n",
      "epoch:31, trian_loss:0.33812, train_acc:0.846, test_loss:0.35548, test_acc:0.826\n",
      "epoch:32, trian_loss:0.33784, train_acc:0.846, test_loss:0.35097, test_acc:0.829\n",
      "epoch:33, trian_loss:0.33774, train_acc:0.846, test_loss:0.34956, test_acc:0.830\n",
      "epoch:34, trian_loss:0.33687, train_acc:0.846, test_loss:0.35685, test_acc:0.825\n",
      "epoch:35, trian_loss:0.33681, train_acc:0.847, test_loss:0.35008, test_acc:0.829\n",
      "epoch:36, trian_loss:0.33617, train_acc:0.847, test_loss:0.34941, test_acc:0.830\n",
      "epoch:37, trian_loss:0.33621, train_acc:0.847, test_loss:0.35092, test_acc:0.828\n",
      "epoch:38, trian_loss:0.33579, train_acc:0.847, test_loss:0.34966, test_acc:0.829\n",
      "epoch:39, trian_loss:0.33552, train_acc:0.847, test_loss:0.34934, test_acc:0.829\n",
      "epoch:40, trian_loss:0.33519, train_acc:0.847, test_loss:0.34854, test_acc:0.830\n",
      "epoch:41, trian_loss:0.33480, train_acc:0.848, test_loss:0.34612, test_acc:0.832\n",
      "epoch:42, trian_loss:0.33463, train_acc:0.848, test_loss:0.34749, test_acc:0.831\n",
      "epoch:43, trian_loss:0.33454, train_acc:0.848, test_loss:0.34774, test_acc:0.831\n",
      "epoch:44, trian_loss:0.33401, train_acc:0.848, test_loss:0.34632, test_acc:0.831\n",
      "epoch:45, trian_loss:0.33376, train_acc:0.848, test_loss:0.34572, test_acc:0.832\n",
      "epoch:46, trian_loss:0.33345, train_acc:0.848, test_loss:0.34539, test_acc:0.831\n",
      "epoch:47, trian_loss:0.33341, train_acc:0.849, test_loss:0.34509, test_acc:0.832\n",
      "epoch:48, trian_loss:0.33294, train_acc:0.849, test_loss:0.34387, test_acc:0.833\n",
      "epoch:49, trian_loss:0.33219, train_acc:0.849, test_loss:0.34230, test_acc:0.833\n",
      "epoch:50, trian_loss:0.33220, train_acc:0.849, test_loss:0.34426, test_acc:0.832\n",
      "epoch:51, trian_loss:0.33219, train_acc:0.849, test_loss:0.34482, test_acc:0.832\n",
      "epoch:52, trian_loss:0.33173, train_acc:0.849, test_loss:0.34405, test_acc:0.832\n",
      "epoch:53, trian_loss:0.33153, train_acc:0.850, test_loss:0.34407, test_acc:0.833\n",
      "epoch:54, trian_loss:0.33134, train_acc:0.849, test_loss:0.34263, test_acc:0.834\n",
      "epoch:55, trian_loss:0.33097, train_acc:0.850, test_loss:0.34263, test_acc:0.834\n",
      "epoch:56, trian_loss:0.33061, train_acc:0.849, test_loss:0.34264, test_acc:0.834\n",
      "epoch:57, trian_loss:0.33054, train_acc:0.849, test_loss:0.34228, test_acc:0.834\n",
      "epoch:58, trian_loss:0.33029, train_acc:0.850, test_loss:0.34220, test_acc:0.834\n",
      "epoch:59, trian_loss:0.33040, train_acc:0.849, test_loss:0.34131, test_acc:0.835\n",
      "epoch:60, trian_loss:0.32960, train_acc:0.850, test_loss:0.34173, test_acc:0.835\n",
      "epoch:61, trian_loss:0.32970, train_acc:0.850, test_loss:0.34108, test_acc:0.835\n",
      "epoch:62, trian_loss:0.32921, train_acc:0.850, test_loss:0.34162, test_acc:0.835\n",
      "epoch:63, trian_loss:0.32911, train_acc:0.851, test_loss:0.34053, test_acc:0.836\n",
      "epoch:64, trian_loss:0.32895, train_acc:0.851, test_loss:0.33992, test_acc:0.837\n",
      "epoch:65, trian_loss:0.32873, train_acc:0.851, test_loss:0.33971, test_acc:0.837\n",
      "epoch:66, trian_loss:0.32866, train_acc:0.851, test_loss:0.33925, test_acc:0.837\n",
      "epoch:67, trian_loss:0.32814, train_acc:0.851, test_loss:0.33800, test_acc:0.838\n",
      "epoch:68, trian_loss:0.32813, train_acc:0.851, test_loss:0.33950, test_acc:0.838\n",
      "epoch:69, trian_loss:0.32797, train_acc:0.851, test_loss:0.33798, test_acc:0.838\n",
      "epoch:70, trian_loss:0.32770, train_acc:0.852, test_loss:0.33868, test_acc:0.838\n",
      "epoch:71, trian_loss:0.32739, train_acc:0.852, test_loss:0.33633, test_acc:0.841\n",
      "epoch:72, trian_loss:0.32717, train_acc:0.852, test_loss:0.33834, test_acc:0.838\n",
      "epoch:73, trian_loss:0.32734, train_acc:0.852, test_loss:0.33720, test_acc:0.840\n",
      "epoch:74, trian_loss:0.32668, train_acc:0.852, test_loss:0.33749, test_acc:0.840\n",
      "epoch:75, trian_loss:0.32680, train_acc:0.853, test_loss:0.33545, test_acc:0.841\n",
      "epoch:76, trian_loss:0.32664, train_acc:0.853, test_loss:0.33669, test_acc:0.841\n",
      "epoch:77, trian_loss:0.32643, train_acc:0.853, test_loss:0.33693, test_acc:0.841\n",
      "epoch:78, trian_loss:0.32615, train_acc:0.853, test_loss:0.33645, test_acc:0.841\n",
      "epoch:79, trian_loss:0.32593, train_acc:0.853, test_loss:0.33683, test_acc:0.840\n",
      "epoch:80, trian_loss:0.32589, train_acc:0.854, test_loss:0.33486, test_acc:0.842\n",
      "epoch:81, trian_loss:0.32560, train_acc:0.854, test_loss:0.33708, test_acc:0.840\n",
      "epoch:82, trian_loss:0.32559, train_acc:0.854, test_loss:0.33440, test_acc:0.842\n",
      "epoch:83, trian_loss:0.32552, train_acc:0.854, test_loss:0.33614, test_acc:0.840\n",
      "epoch:84, trian_loss:0.32514, train_acc:0.854, test_loss:0.33639, test_acc:0.840\n",
      "epoch:85, trian_loss:0.32515, train_acc:0.855, test_loss:0.33596, test_acc:0.840\n",
      "epoch:86, trian_loss:0.32490, train_acc:0.854, test_loss:0.33467, test_acc:0.841\n",
      "epoch:87, trian_loss:0.32471, train_acc:0.855, test_loss:0.33663, test_acc:0.839\n",
      "epoch:88, trian_loss:0.32439, train_acc:0.855, test_loss:0.33592, test_acc:0.840\n",
      "epoch:89, trian_loss:0.32443, train_acc:0.855, test_loss:0.33539, test_acc:0.840\n",
      "epoch:90, trian_loss:0.32430, train_acc:0.855, test_loss:0.33357, test_acc:0.842\n",
      "epoch:91, trian_loss:0.32427, train_acc:0.855, test_loss:0.33466, test_acc:0.841\n",
      "epoch:92, trian_loss:0.32398, train_acc:0.855, test_loss:0.33505, test_acc:0.841\n",
      "epoch:93, trian_loss:0.32388, train_acc:0.855, test_loss:0.33441, test_acc:0.842\n",
      "epoch:94, trian_loss:0.32354, train_acc:0.856, test_loss:0.33482, test_acc:0.842\n",
      "epoch:95, trian_loss:0.32342, train_acc:0.855, test_loss:0.33375, test_acc:0.842\n",
      "epoch:96, trian_loss:0.32316, train_acc:0.856, test_loss:0.33305, test_acc:0.843\n",
      "epoch:97, trian_loss:0.32294, train_acc:0.856, test_loss:0.33280, test_acc:0.843\n",
      "epoch:98, trian_loss:0.32289, train_acc:0.856, test_loss:0.33279, test_acc:0.843\n",
      "epoch:99, trian_loss:0.32252, train_acc:0.857, test_loss:0.33271, test_acc:0.843\n",
      "epoch:100, trian_loss:0.32226, train_acc:0.856, test_loss:0.33204, test_acc:0.843\n",
      "epoch:101, trian_loss:0.32226, train_acc:0.856, test_loss:0.32986, test_acc:0.845\n",
      "epoch:102, trian_loss:0.32189, train_acc:0.857, test_loss:0.33166, test_acc:0.844\n",
      "epoch:103, trian_loss:0.32196, train_acc:0.857, test_loss:0.33067, test_acc:0.844\n",
      "epoch:104, trian_loss:0.32150, train_acc:0.857, test_loss:0.33017, test_acc:0.845\n",
      "epoch:105, trian_loss:0.32114, train_acc:0.857, test_loss:0.32946, test_acc:0.845\n",
      "epoch:106, trian_loss:0.32096, train_acc:0.858, test_loss:0.32853, test_acc:0.846\n",
      "epoch:107, trian_loss:0.32076, train_acc:0.858, test_loss:0.32925, test_acc:0.845\n",
      "epoch:108, trian_loss:0.32068, train_acc:0.858, test_loss:0.32942, test_acc:0.845\n",
      "epoch:109, trian_loss:0.32011, train_acc:0.859, test_loss:0.32808, test_acc:0.847\n",
      "epoch:110, trian_loss:0.31995, train_acc:0.859, test_loss:0.32957, test_acc:0.845\n",
      "epoch:111, trian_loss:0.31959, train_acc:0.859, test_loss:0.32920, test_acc:0.846\n",
      "epoch:112, trian_loss:0.31954, train_acc:0.859, test_loss:0.32923, test_acc:0.846\n",
      "epoch:113, trian_loss:0.31920, train_acc:0.860, test_loss:0.32943, test_acc:0.846\n",
      "epoch:114, trian_loss:0.31901, train_acc:0.860, test_loss:0.32832, test_acc:0.847\n",
      "epoch:115, trian_loss:0.31880, train_acc:0.860, test_loss:0.32941, test_acc:0.847\n",
      "epoch:116, trian_loss:0.31856, train_acc:0.860, test_loss:0.32946, test_acc:0.847\n",
      "epoch:117, trian_loss:0.31820, train_acc:0.860, test_loss:0.32817, test_acc:0.847\n",
      "epoch:118, trian_loss:0.31832, train_acc:0.860, test_loss:0.32914, test_acc:0.848\n",
      "epoch:119, trian_loss:0.31790, train_acc:0.861, test_loss:0.32934, test_acc:0.847\n",
      "epoch:120, trian_loss:0.31784, train_acc:0.860, test_loss:0.32888, test_acc:0.847\n",
      "epoch:121, trian_loss:0.31747, train_acc:0.861, test_loss:0.32902, test_acc:0.847\n",
      "epoch:122, trian_loss:0.31737, train_acc:0.860, test_loss:0.32789, test_acc:0.848\n",
      "epoch:123, trian_loss:0.31728, train_acc:0.861, test_loss:0.32889, test_acc:0.847\n",
      "epoch:124, trian_loss:0.31721, train_acc:0.861, test_loss:0.32832, test_acc:0.848\n",
      "epoch:125, trian_loss:0.31697, train_acc:0.860, test_loss:0.32773, test_acc:0.849\n",
      "epoch:126, trian_loss:0.31703, train_acc:0.861, test_loss:0.32843, test_acc:0.848\n",
      "epoch:127, trian_loss:0.31670, train_acc:0.860, test_loss:0.32759, test_acc:0.849\n",
      "epoch:128, trian_loss:0.31661, train_acc:0.860, test_loss:0.32828, test_acc:0.849\n",
      "epoch:129, trian_loss:0.31655, train_acc:0.860, test_loss:0.32807, test_acc:0.849\n",
      "epoch:130, trian_loss:0.31623, train_acc:0.860, test_loss:0.32809, test_acc:0.849\n",
      "epoch:131, trian_loss:0.31623, train_acc:0.860, test_loss:0.32738, test_acc:0.849\n",
      "epoch:132, trian_loss:0.31625, train_acc:0.861, test_loss:0.32732, test_acc:0.849\n",
      "epoch:133, trian_loss:0.31576, train_acc:0.860, test_loss:0.32767, test_acc:0.849\n",
      "epoch:134, trian_loss:0.31581, train_acc:0.860, test_loss:0.32805, test_acc:0.849\n",
      "epoch:135, trian_loss:0.31560, train_acc:0.860, test_loss:0.32807, test_acc:0.849\n",
      "epoch:136, trian_loss:0.31558, train_acc:0.860, test_loss:0.32826, test_acc:0.849\n",
      "epoch:137, trian_loss:0.31528, train_acc:0.860, test_loss:0.32849, test_acc:0.849\n",
      "epoch:138, trian_loss:0.31514, train_acc:0.860, test_loss:0.32812, test_acc:0.849\n",
      "epoch:139, trian_loss:0.31496, train_acc:0.860, test_loss:0.32826, test_acc:0.849\n",
      "epoch:140, trian_loss:0.31479, train_acc:0.860, test_loss:0.32773, test_acc:0.849\n",
      "epoch:141, trian_loss:0.31483, train_acc:0.861, test_loss:0.32830, test_acc:0.848\n",
      "epoch:142, trian_loss:0.31459, train_acc:0.860, test_loss:0.32985, test_acc:0.847\n",
      "epoch:143, trian_loss:0.31468, train_acc:0.860, test_loss:0.32839, test_acc:0.848\n",
      "epoch:144, trian_loss:0.31428, train_acc:0.860, test_loss:0.32855, test_acc:0.847\n",
      "epoch:145, trian_loss:0.31439, train_acc:0.861, test_loss:0.32808, test_acc:0.848\n",
      "epoch:146, trian_loss:0.31390, train_acc:0.860, test_loss:0.32865, test_acc:0.848\n",
      "epoch:147, trian_loss:0.31368, train_acc:0.861, test_loss:0.32824, test_acc:0.849\n",
      "epoch:148, trian_loss:0.31381, train_acc:0.861, test_loss:0.32866, test_acc:0.849\n",
      "epoch:149, trian_loss:0.31363, train_acc:0.860, test_loss:0.32883, test_acc:0.849\n",
      "epoch:150, trian_loss:0.31341, train_acc:0.860, test_loss:0.32834, test_acc:0.849\n",
      "epoch:151, trian_loss:0.31327, train_acc:0.860, test_loss:0.32887, test_acc:0.849\n",
      "epoch:152, trian_loss:0.31298, train_acc:0.861, test_loss:0.32805, test_acc:0.850\n",
      "epoch:153, trian_loss:0.31283, train_acc:0.861, test_loss:0.32835, test_acc:0.850\n",
      "epoch:154, trian_loss:0.31264, train_acc:0.861, test_loss:0.32837, test_acc:0.850\n",
      "epoch:155, trian_loss:0.31247, train_acc:0.862, test_loss:0.32852, test_acc:0.850\n",
      "epoch:156, trian_loss:0.31223, train_acc:0.862, test_loss:0.32890, test_acc:0.850\n",
      "epoch:157, trian_loss:0.31209, train_acc:0.862, test_loss:0.32913, test_acc:0.850\n",
      "epoch:158, trian_loss:0.31195, train_acc:0.863, test_loss:0.32909, test_acc:0.850\n",
      "epoch:159, trian_loss:0.31177, train_acc:0.863, test_loss:0.32915, test_acc:0.850\n",
      "epoch:160, trian_loss:0.31143, train_acc:0.863, test_loss:0.32894, test_acc:0.850\n",
      "epoch:161, trian_loss:0.31143, train_acc:0.863, test_loss:0.32913, test_acc:0.849\n",
      "epoch:162, trian_loss:0.31131, train_acc:0.864, test_loss:0.32947, test_acc:0.849\n",
      "epoch:163, trian_loss:0.31098, train_acc:0.864, test_loss:0.32987, test_acc:0.850\n",
      "epoch:164, trian_loss:0.31087, train_acc:0.864, test_loss:0.32986, test_acc:0.850\n",
      "epoch:165, trian_loss:0.31060, train_acc:0.864, test_loss:0.33130, test_acc:0.849\n",
      "epoch:166, trian_loss:0.31037, train_acc:0.864, test_loss:0.33000, test_acc:0.851\n",
      "epoch:167, trian_loss:0.31021, train_acc:0.865, test_loss:0.32997, test_acc:0.850\n",
      "epoch:168, trian_loss:0.31001, train_acc:0.865, test_loss:0.33005, test_acc:0.850\n",
      "epoch:169, trian_loss:0.30973, train_acc:0.865, test_loss:0.32995, test_acc:0.851\n",
      "epoch:170, trian_loss:0.30930, train_acc:0.866, test_loss:0.33017, test_acc:0.850\n",
      "epoch:171, trian_loss:0.30894, train_acc:0.865, test_loss:0.32990, test_acc:0.850\n",
      "epoch:172, trian_loss:0.30881, train_acc:0.866, test_loss:0.32993, test_acc:0.850\n",
      "epoch:173, trian_loss:0.30854, train_acc:0.866, test_loss:0.32924, test_acc:0.850\n",
      "epoch:174, trian_loss:0.30825, train_acc:0.866, test_loss:0.32996, test_acc:0.849\n",
      "epoch:175, trian_loss:0.30802, train_acc:0.866, test_loss:0.32980, test_acc:0.849\n",
      "epoch:176, trian_loss:0.30774, train_acc:0.866, test_loss:0.32976, test_acc:0.849\n",
      "epoch:177, trian_loss:0.30779, train_acc:0.867, test_loss:0.32993, test_acc:0.849\n",
      "epoch:178, trian_loss:0.30730, train_acc:0.867, test_loss:0.33003, test_acc:0.849\n",
      "epoch:179, trian_loss:0.30721, train_acc:0.867, test_loss:0.33004, test_acc:0.849\n",
      "epoch:180, trian_loss:0.30708, train_acc:0.867, test_loss:0.33020, test_acc:0.849\n",
      "epoch:181, trian_loss:0.30696, train_acc:0.867, test_loss:0.33036, test_acc:0.849\n",
      "epoch:182, trian_loss:0.30675, train_acc:0.867, test_loss:0.33021, test_acc:0.850\n",
      "epoch:183, trian_loss:0.30657, train_acc:0.866, test_loss:0.33050, test_acc:0.850\n",
      "epoch:184, trian_loss:0.30641, train_acc:0.867, test_loss:0.33055, test_acc:0.850\n",
      "epoch:185, trian_loss:0.30634, train_acc:0.867, test_loss:0.33071, test_acc:0.850\n",
      "epoch:186, trian_loss:0.30625, train_acc:0.867, test_loss:0.33078, test_acc:0.850\n",
      "epoch:187, trian_loss:0.30611, train_acc:0.867, test_loss:0.33092, test_acc:0.850\n",
      "epoch:188, trian_loss:0.30590, train_acc:0.867, test_loss:0.33106, test_acc:0.850\n",
      "epoch:189, trian_loss:0.30590, train_acc:0.867, test_loss:0.33126, test_acc:0.850\n",
      "epoch:190, trian_loss:0.30574, train_acc:0.867, test_loss:0.33117, test_acc:0.850\n",
      "epoch:191, trian_loss:0.30567, train_acc:0.868, test_loss:0.33139, test_acc:0.850\n",
      "epoch:192, trian_loss:0.30547, train_acc:0.868, test_loss:0.33160, test_acc:0.850\n",
      "epoch:193, trian_loss:0.30532, train_acc:0.868, test_loss:0.33165, test_acc:0.850\n",
      "epoch:194, trian_loss:0.30526, train_acc:0.868, test_loss:0.33175, test_acc:0.849\n",
      "epoch:195, trian_loss:0.30518, train_acc:0.868, test_loss:0.33196, test_acc:0.849\n",
      "epoch:196, trian_loss:0.30493, train_acc:0.868, test_loss:0.33211, test_acc:0.849\n",
      "epoch:197, trian_loss:0.30498, train_acc:0.868, test_loss:0.33214, test_acc:0.850\n",
      "epoch:198, trian_loss:0.30486, train_acc:0.868, test_loss:0.33230, test_acc:0.850\n",
      "epoch:199, trian_loss:0.30460, train_acc:0.868, test_loss:0.33236, test_acc:0.850\n",
      "epoch:200, trian_loss:0.30448, train_acc:0.868, test_loss:0.33241, test_acc:0.849\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(config.epochs):\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "    for X, y in data_loader:\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X)\n",
    "            l = loss(y_true=tf.one_hot(y, depth=2), y_pred=y_pred)\n",
    "        grads = tape.gradient(l, model.trainable_variables)\n",
    "        # print(grads)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        train_loss_list.append(l)\n",
    "        train_acc_list.append(evaluate(y_true=y, y_pred=model(X)))\n",
    "        # train_acc_list.append(evaluate(y_true=y, y_pred=tf.squeeze(model(X))))\n",
    "    train_loss = np.mean(train_loss_list)\n",
    "    train_acc = np.mean(train_acc_list)\n",
    "    test_loss = loss(y_pred=model(X_test.toarray()), y_true=tf.one_hot(y_test, depth=2))\n",
    "    # test_loss = loss(y_pred=tf.squeeze(model(X_test.toarray())), y_true=y_test)\n",
    "    test_acc = evaluate(y_pred=model(X_test.toarray()), y_true=y_test)\n",
    "    print(\"epoch:{}, trian_loss:{:.5f}, train_acc:{:.3f}, test_loss:{:.5f}, test_acc:{:.3f}\".format(epoch+1, train_loss, train_acc, test_loss.numpy(), test_acc))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
